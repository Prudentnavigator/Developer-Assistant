        *************** Modelfile Help ***************


Format:

The format of the Modelfile:

# comment
INSTRUCTION arguments

Instruction 	Description
FROM (required) 	Defines the base model to use.
PARAMETER 	Sets the parameters for how Ollama will run the model.
TEMPLATE 	The full prompt template to be sent to the model.
SYSTEM   	Specifies the system message that will be set in the template.
ADAPTER 	Defines the (Q)LoRA adapters to apply to the model.
LICENSE 	Specifies the legal license.
MESSAGE 	Specify message history.

-------------------------------------------------------------------------------
Basic Modelfile:

An example of a Modelfile creating a developer assistant blueprint:

FROM llama3.2
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# sets the context window size to 4096, this controls how many tokens the LLM
# can use as context to generate the next token.
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM """You are a professional developer assistant that answers concisely."""

-------------------------------------------------------------------------------


###############################################################################

    VERY IMPORTANT: Please only leave one space after the keyword! 

e.g. PARAMETER seed 1       # also one space between param and param value.
e.g  SYSTEM """<system message>"""

###############################################################################


--> Instructions:

FROM (Required)

The FROM instruction defines the base model to use when creating a model.

FROM <model name>:<tag>


--> PARAMETER:

The PARAMETER instruction defines a parameter that can be set when the model is run.

PARAMETER <parameter> <parametervalue>

Valid Parameters and Values:

mirostat -- Enable Mirostat sampling for controlling perplexity.
             (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
             type=int       e.g. mirostat 0

mirostat_eta -- Influences how quickly the algorithm responds to feedback from
                the generated text. A lower learning rate will result in slower
                adjustments, while a higher learning rate will make the algorithm
                more responsive.
                 (Default: 0.1)
                 type=float     e.g. mirostat_eta 0.1

mirostat_tau -- Controls the balance between coherence and diversity of the output.
                A lower value will result in more focused and coherent text.
                 (Default: 5.0)
                 type=float      e.g.mirostat_tau 5.0

num_ctx -- Sets the size of the context window used to generate the next token.
            (Default: 2048)
            type=int 	e.g. num_ctx 4096

repeat_last_n -- Sets how far back for the model to look back to prevent repetition.
                  (Default: 64, 0 = disabled, -1 = num_ctx)
                  type=int 	e.g. repeat_last_n 64

repeat_penalty -- Sets how strongly to penalize repetitions. A higher value (e.g., 1.5)
                  will penalize repetitions more strongly, while a lower value
                  (e.g., 0.9) will be more lenient.
                   (Default: 1.1)
                   type=float 	e.g. repeat_penalty 1.1


temperature -- The temperature of the model. Increasing the temperature will make
               the model answer more creatively.
                (Default: 0.8)
                type=float 	 e.g. temperature 0.7


seed --	Sets the random number seed to use for generation. Setting this to a specific
        number will make the model generate the same text for the same prompt.
         (Default: 0)
         type=int 	e.g. seed 42


stop --	Sets the stop sequences to use. When this pattern is encountered the LLM will
        stop generating text and return. Multiple stop patterns may be set by
        specifying multiple separate stop parameters in a modelfile.
         type=string 	e.g. stop "AI assistant:"


tfs_z -- Tail free sampling is used to reduce the impact of less probable tokens from
         the output. A higher value (e.g., 2.0) will reduce the impact more, while a
         value of 1.0 disables this setting.
          (default: 1) 	
          type=float 	e.g. tfs_z 1


num_predict -- Maximum number of tokens to predict when generating text.
                (Default: 128, -1 = infinite generation, -2 = fill context)
                type=int 	e.g. num_predict 42


top_k -- Reduces the probability of generating nonsense. A higher value (e.g. 100)
         will give more diverse answers, while a lower value (e.g. 10) will be
         more conservative. 
          (Default: 40)
          type=int 	e.g. top_k 40


top_p -- Works together with top-k. A higher value (e.g., 0.95) will lead to more
         diverse text, while a lower value (e.g., 0.5) will generate more focused
         and conservative text.
          (Default: 0.9)
          type=float 	e.g. top_p 0.9


min_p -- Alternative to the top_p, and aims to ensure a balance of quality and variety.
         The parameter p represents the minimum probability for a token to be considered,
         relative to the probability of the most likely token. For example, with p=0.05
         and the most likely token having a probability of 0.9, logits with a value less
         than 0.045 are filtered out.
          (Default: 0.0)
          type=float 	e.g. min_p 0.05


--> TEMPLATE:

TEMPLATE of the full prompt template to be passed into the model. It may include
(optionally) a system message, a user's message and the response from the model.
Note: syntax may be model specific. Templates use Go template syntax.

Template Variables:

Variable 	Description
{{ .System }} 	The system message used to specify custom behavior.
{{ .Prompt }} 	The user prompt message.
{{ .Response }} 	The response from the model. When generating a response,
                    text after this variable is omitted.

Example:

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
 

--> SYSTEM:

The SYSTEM instruction specifies the system message to be used in the template,
if applicable.

Example:

SYSTEM """<system message>"""


--> LICENSE:

The LICENSE instruction allows you to specify the legal license under which the model
used with this Modelfile is shared or distributed.

Example:

LICENSE """
<license text>
"""
