#!/bin/env python3
# -*- mode: python; coding: utf-8 -*-

# Script by Thomas Pirchmoser (tommy_software@mailfence.com) 2025

# This script was created for personal/educational purposes only and is not to
#   be used for commercial or profit purposes.

'''
dev_assist.py--This module provides an AI assistant for programming-related
queries or any other topics. It uses OLLAMA API to generate responses based
on user input and appends it to the chat log. The 'ask()' function takes in
a query, language model name, and an optional file path as arguments and
returns the response generated by the AI assistant.
'''

import ollama
from utils_dev_assist.dev_assist_logging import app_log

logger = app_log(__name__)

# Initializing an empty list for storing the chat messages.
chat_messages = []


def create_message(message: str, role: str) -> dict:
    """
    Creates a new message with specified roles ('user' or 'assistant').
    This function is used to generate the messages that will be sent to the
    OLLAMA server.

    Args:
        message (str): The content of this particular user-provided text which
                        should include a question, statement etc., related with
                        programming or any other topic you want AI assistant to
                        assist in the given context.

        role(str) : Role can be either 'user' for users who are asking
                    questions and need assistance from the program, or it
                    could also represent the "assistant".
    Returns:
        dict : A dictionary containing 'role' as key for role and message
                content in value.
    """

    return {'role': role,
            'content': message}


def chat(llm: str) -> str:
    '''
    Send a message to OLLAMA's server. The function returns response from
    AI assistant in JSON format if successful, otherwise an error string is
    returned with traceback information for debugging purposes.

    Args:
        llm (str) : Model name of the chosen language model and it's used to
                    identify the model in API requests.

    Returns:
        str (JSON) : Response from OLLAMA server if successful else an error
                     message with traceback information for debugging purposes.
    '''

    try:
        logger.info(" [QUERY] ollama chat query --> send.")

        # Calling the ollama API to get the assistant response.
        ollama_response = ollama.chat(model=llm,
                                      messages=chat_messages,
                                      options={'temperature': 0})

    except Exception as error:
        logger.error(" [RESPONSE] --> %s ", error)

        # Return error message to the GUI.
        return f"\n\n[Error]  -->  {error}!\n\n"

    else:
        # Preparing the assistant message by concatenating the response
        # from the API.
        assistant_message = ""
        assistant_message += ollama_response["message"]["content"]

        # Adding the response message to the chat log.
        chat_messages.append(create_message(assistant_message, "assistant"))

        logger.info(" [RESPONSE] ollama chat response --> returned!")

        # Return the API response.
        response = f"\n{ollama_response['message']['content']}\n"
        return f"\n\u27BE Response: {response}\n"


def ask(query: str, llm: str, add_file: str) -> str:
    """
    The 'ask()' method is used to send messages to the chat().

    Args:
        query(str): The content for this particular user-provided text which
                    should include a question, statement etc., to be asked.
        llm(str) : Model name of the chosen language model.
        add_file(str) : If provided with a file path, this function will
                        append content from that specific text/document to
                        the query.

    Returns:
        response(str):  The response generated from the chat.
    """

    # If a file is attached to the query, read and add the files
    # content to the query.
    if add_file != "":
        with open(add_file, "r", encoding="utf-8") as file:
            query = f"{query} \n {file.read()}"

        logger.info(" %s has been attached to the request.", add_file)

    # Append user query to the chat_messages.
    chat_messages.append(create_message(query, "user"))
    # Send the query to the API and store the resopnse.
    response = chat(llm)

    # Return the API response to the GUI.
    return response
