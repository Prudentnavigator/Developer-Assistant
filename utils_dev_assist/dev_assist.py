#!/bin/env python3
# -*- mode: python; coding: utf-8 -*-

# Script by Thomas Pirchmoser (tommy_software@mailfence.com) 2025

# This script was created for personal/educational purposes only and is not to
#   be used for commercial or profit purposes.

'''
dev_assist.py--This module provides an AI assistant for programming-related
               queries or any other topics. It uses OLLAMA API to generate
               responses based on user input and appends it to the chat log.
               The 'ask()' function takes in a query, language model name,
               and an optional file path as arguments and returns the response
               generated by the AI assistant.
'''

import ollama
import httpx
from utils_dev_assist.dev_assist_logging import app_log

logger = app_log(__name__)

# Initializing an empty list for storing the chat messages.
chat_messages = []


def create_message(message: str, role: str) -> dict:
    """
    Creates a new message with specified roles ('user' or 'assistant').
    This function is used to generate the messages that will be sent to the
    OLLAMA server.
    """

    return {'role': role,
            'content': message}


def chat(llm: str) -> str:
    '''
    Send a message to OLLAMA's server. The function returns response from
    AI assistant in JSON format if successful, otherwise an error string is
    returned with traceback information for debugging purposes.
    '''

    try:
        logger.info(" [QUERY] ollama chat query --> send.")

        # Calling the ollama API to get the assistant response.
        ollama_response = ollama.chat(model=llm,
                                      messages=chat_messages)

    except ollama._types.ResponseError as error:
        logger.error(" [RESPONSE]  --> %s", error.error)
        error_msg = error.error.split("/")
        return f"{error_msg[0]}!"

    except httpx.RemoteProtocolError:
        return "error:\nServer disconnected without sending a response!"

    except Exception as error:
        return f"error:\n{error}"

    # Preparing the assistant message by concatenating the response
    # from the API.
    assistant_message = ""
    assistant_message += ollama_response["message"]["content"]

    # Adding the response message to the chat log.
    chat_messages.append(create_message(assistant_message, "assistant"))

    logger.info(" [RESPONSE] ollama chat response --> returned!")

    # Return the API response.
    response = f"\n{ollama_response['message']['content']}\n"
    return f"\n\u27BE Response: {response}\n"


def ask(query: str, llm: str, add_file: str) -> str:
    """
    The 'ask()' function is used to send messages to the chat().
    """

    # If a file is attached to the query, read and add the files
    # content to the query.
    if add_file != "":
        with open(add_file, "r", encoding="utf-8") as file:
            query = f"{query} \n {file.read()}"

        logger.info(" %s has been attached to the request.", add_file)

    # Append user query to the chat_messages.
    chat_messages.append(create_message(query, "user"))
    # Send the query to the API and store the response.
    response = chat(llm)

    # Return the API response to the GUI.
    return response
